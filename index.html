<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <title>FastVLM WebGPU (Minimal)</title>
  <style>
    html,body { margin:0; height:100%; background:#0b0f17; color:#e6edf3; font:16px/1.5 system-ui,sans-serif }
    .wrap { display:grid; grid-template-rows:auto 1fr auto; min-height:100dvh; }
    header,footer { opacity:.7; padding:12px 16px }
    main { display:grid; place-items:center; padding:16px }
    .card { width:min(900px,100%); backdrop-filter: blur(12px) saturate(1.2); background-color: rgba(255,255,255,.06); border:1px solid rgba(255,255,255,.1); border-radius:16px; padding:16px; }
    .row { display:flex; gap:12px; align-items:center; flex-wrap:wrap }
    video { width:100%; max-height:60vh; border-radius:12px; background:#000; object-fit:cover }
    textarea { width:100%; height:76px; resize:vertical; border-radius:8px; padding:8px; border:1px solid rgba(255,255,255,.15); background:#0f1624; color:#e6edf3 }
    button { padding:10px 14px; border-radius:10px; background:#1f6feb; color:white; border:0; cursor:pointer }
    .badge { font-size:12px; opacity:.8 }
    .mono { font-family: ui-monospace, SFMono-Regular, Menlo, Consolas, "Liberation Mono", monospace }
    .out { white-space:pre-wrap; background:#0f1624; border:1px solid rgba(255,255,255,.08); padding:12px; border-radius:8px; min-height:3lh }
    .dim { opacity:.7 }
  </style>
</head>
<body>
  <div class="wrap">
    <header>
      <div class="row"><strong>FastVLM WebGPU (Minimal)</strong><span class="badge">apple/FastVLM-0.5B (ONNX)</span></div>
    </header>

    <main>
      <div class="card">
        <div class="row" style="justify-content:space-between">
          <div id="status" class="mono dim">Status: idle</div>
          <div id="perf" class="mono dim"></div>
        </div>

        <video id="webcam" autoplay muted playsinline></video>

        <div class="row" style="margin-top:12px">
          <textarea id="instruction" placeholder="Ask the model while it sees the video (e.g., 'Describe what you see')"></textarea>
        </div>

        <div class="row" style="justify-content:space-between">
          <div class="row">
            <button id="btn-permission">Grant webcam</button>
            <button id="btn-load" disabled>Load model</button>
            <button id="btn-start" disabled>Start captioning</button>
            <button id="btn-stop" disabled>Stop</button>
          </div>
          <div class="mono dim">Device: <span id="dev">detecting…</span></div>
        </div>

        <h4>Output</h4>
        <div id="output" class="out"></div>
      </div>
    </main>

    <footer class="dim">Runs fully in your browser via WebGPU + transformers.js</footer>
  </div>

  <script type="module">
    import {
      AutoProcessor,
      AutoModelForImageTextToText,
      RawImage,
      TextStreamer,
    } from "https://cdn.jsdelivr.net/npm/@huggingface/transformers@3.0.0/dist/transformers.min.js";

    const MODEL_ID = "onnx-community/FastVLM-0.5B-ONNX"; // same family used in the Space
    const status = (t) => (document.getElementById("status").textContent = "Status: " + t);
    const perf = (t) => (document.getElementById("perf").textContent = t ?? "");
    const devSpan = document.getElementById("dev");
    const video = document.getElementById("webcam");
    const out = document.getElementById("output");
    const btnPerm = document.getElementById("btn-permission");
    const btnLoad = document.getElementById("btn-load");
    const btnStart = document.getElementById("btn-start");
    const btnStop = document.getElementById("btn-stop");
    const instructionEl = document.getElementById("instruction");

    // Detect WebGPU
    const hasWebGPU = !!navigator.gpu;
    devSpan.textContent = hasWebGPU ? "webgpu" : "wasm (fallback)";
    if (!hasWebGPU) console.warn("WebGPU not available—Chrome/Edge 113+ recommended.");

    let processor, model, stream, running = false, inferenceLock = false;
    const canvas = document.createElement("canvas");
    const ctx = canvas.getContext("2d", { willReadFrequently: true });

    btnPerm.onclick = async () => {
      try {
        status("requesting webcam");
        stream = await navigator.mediaDevices.getUserMedia({ video: { facingMode: "user" }, audio: false });
        video.srcObject = stream;
        await video.play();
        btnLoad.disabled = false;
        status("webcam granted");
      } catch (e) {
        status("webcam denied");
        console.error(e);
      }
    };

    btnLoad.onclick = async () => {
      try {
        status("loading processor…");
        processor = await AutoProcessor.from_pretrained(MODEL_ID);
        status("loading model…");
        model = await AutoModelForImageTextToText.from_pretrained(MODEL_ID, {
          // mirror Space settings: run on WebGPU with lightweight dtypes/quant
          dtype: { embed_tokens: "fp16", vision_encoder: "q4", decoder_model_merged: "q4" },
          device: hasWebGPU ? "webgpu" : "wasm",
        });
        btnStart.disabled = false;
        status("model loaded");
      } catch (e) {
        status("load failed");
        console.error(e);
      }
    };

    btnStart.onclick = () => {
      running = true;
      btnStop.disabled = false;
      status("captioning…");
      loop();
    };
    btnStop.onclick = () => {
      running = false;
      status("stopped");
    };

    async function inferOne(instruction) {
      if (inferenceLock || !processor || !model) return;
      inferenceLock = true;

      // Draw current frame
      canvas.width = video.videoWidth || 640;
      canvas.height = video.videoHeight || 480;
      ctx.drawImage(video, 0, 0, canvas.width, canvas.height);
      const frame = ctx.getImageData(0, 0, canvas.width, canvas.height);
      const rawImg = new RawImage(frame.data, frame.width, frame.height, 4);

      // Build chat-style prompt with the <image> token
      const messages = [
        { role: "system", content: "You are a helpful visual AI. Answer concisely in one sentence." },
        { role: "user", content: `<image>${instruction || "Describe the scene."}` },
      ];
      const prompt = processor.apply_chat_template(messages, { add_generation_prompt: true });

      const t0 = performance.now();
      const inputs = await processor(rawImg, prompt, { add_special_tokens: false });

      let streamed = "";
      const streamer = new TextStreamer(processor.tokenizer, {
        skip_prompt: true,
        skip_special_tokens: true,
        callback_function: (t) => {
          streamed += t;
          out.textContent = streamed.trim();
        },
      });

      const outputs = await model.generate({
        ...inputs,
        max_new_tokens: 128,
        do_sample: false,
        repetition_penalty: 1.2,
        streamer,
      });

      // If you want the final decoded string (not just streaming):
      // const decoded = processor.batch_decode(
      //   outputs.slice(null, [inputs.input_ids.dims.at(-1), null]),
      //   { skip_special_tokens: true }
      // )[0];

      const t1 = performance.now();
      perf(`last frame: ${(t1 - t0).toFixed(0)} ms`);
      inferenceLock = false;
    }

    async function loop() {
      if (!running) return;
      // Throttle: run about every 1.5s
      await inferOne(instructionEl.value);
      setTimeout(loop, 1500);
    }

    status("idle");
  </script>
</body>
</html>
